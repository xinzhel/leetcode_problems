{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mote Carlo**\n",
    "The essence is the update equation:\n",
    "$$\\hat{Q}_{\\mathrm{opt}}(s, a)=\\sum_{s^{\\prime}} \\hat{T}\\left(s, a, s^{\\prime}\\right)\\left[\\widehat{\\operatorname{Reward}}\\left(s, a, s^{\\prime}\\right)+\\gamma \\hat{V}_{\\mathrm{opt}}\\left(s^{\\prime}\\right)\\right]$$\n",
    "Here's a breakdown of the terms in the equation:\n",
    "1. $\\hat{Q}_{\\text {opt }}(s, a)$ : The estimated optimal Q-value for state $s$ and action $a$.\n",
    "2. $\\hat{T}\\left(s, a, s^{\\prime}\\right)$ : The estimated transition probability of reaching state $s^{\\prime}$ after taking action $a$ in state $s$.\n",
    "3. $\\overline{\\operatorname{Reward}}\\left(s, a, s^{\\prime}\\right)$ : The estimated reward received after transitioning from state $s$ to state $s^{\\prime}$ by taking action $a$.\n",
    "4. $\\gamma$ : The discount factor, which determines the present value of future rewards. It's a number between 0 and 1 . When $\\gamma$ is closer to 1 , the agent gives more weight to future rewards, and when it's closer to 0 , it prioritizes immediate rewards.\n",
    "5. $\\hat{V}_{\\text {opt }}\\left(s^{\\prime}\\right)$ : The estimated optimal value function for the next state $s^{\\prime}$. This represents the maximum expected cumulative reward from state $s^{\\prime}$ onwards, assuming the agent acts optimally.\n",
    "\n",
    "* Model-based Monk Corlo $\\hat{T}$, $\\widehat{\\operatorname{Reward}}$ \n",
    "* Model-free Monte Carlo $\\hat{Q}_\\pi$\n",
    "\n",
    "## **Q-learning**\n",
    "Q-learning is a form of Temporal Difference (TD) learning, and it can be conceptualized as a form of value iteration for the Q-values (or action values) of a Markov Decision Process (MDP). Let's break down how Q-learning relates to value iteration using the provided code:\n",
    "Q-learning Algorithm:\n",
    "\n",
    "\n",
    "The equation you've provided is related to the concept of Q-learning, a popular model-free reinforcement learning algorithm. The function $\\hat{Q}_{\\text {opt }}(s, a)$ represents the estimated optimal Q-value for a given state-action pair $(s, a)$. The Q-value quantifies the expected cumulative reward of taking action $a$ in state $s$ and then acting optimally thereafter.\n",
    "\n",
    "The equation is basically a representation of the Bellman optimality equation for Q-values. It states that the optimal Q-value for a state-action pair $(s, a)$ is the expected reward for taking action $a$ in state $s$, plus the discounted optimal value of the next state $s^{\\prime}$.\n",
    "In simpler terms, the equation captures the idea that the value of a given action in a state is the immediate reward from that action plus the value of the best possible future actions. By recursively updating the $\\mathrm{Q}$-values using this equation, the agent learns to act optimally to maximize its expected cumulative reward.\n",
    "\n",
    "**How to find optimal policy after learning Q-value**: Once Q-values are learned, the optimal policy can be derived directly from the Q-values by selecting the action that maximizes the Q-value for each state.\n",
    "\n",
    "## **Value iteration**\n",
    "The essence of value iteration is to iteratively refine the value function using the Bellman optimality equation:\n",
    "$$\n",
    "V(s) \\leftarrow \\max _a\\left[R(s, a)+\\gamma \\sum_{s^{\\prime}} P\\left(s^{\\prime} \\mid s, a\\right) V\\left(s^{\\prime}\\right)\\right]\n",
    "$$\n",
    "Where $V(s)$ is the value of state $s, R(s, a)$ is the reward for taking action $a$ in state $s$, and $P\\left(s^{\\prime} \\mid s, a\\right)$ is the transition probability.\n",
    "Linking Q-learning to Value Iteration:\n",
    "1. Iterative Update: Both Q-learning and Value Iteration algorithms involve iteratively updating their respective value estimates until they converge to the optimal values. In Qlearning, it's the Q-values, while in Value Iteration, it's the state values.\n",
    "2. Optimality:\n",
    "- In Value Iteration, the value of a state $s$ is updated based on the maximum expected value over all possible action by using the Bellman optimality equation until convergence\n",
    "- In Q-learning, the Q-value for a state-action pair $(s, a)$ is updated based on the maximum Q-value of the next state $s^{\\prime}$ across all actions, which is the term $\\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1: Modify the code for tic-tac-toe that you designed in Modules 1-3, and code another algorithm to solve it. You are expected to:\n",
    "- (Optional Step) Formulate the problem as an MDP, that means, you will have a graph representing each board as a state.\n",
    "- Code a solution to solve the MDP using **Monte-Carlo RL algorithm** (Make sure you do Exploring Start).\n",
    "- Devise and code a solution to tic-tac-toe using **Temporal Difference algorithms - SARSA** and **Q-Learning**. Compare the convergence performance of two algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Problem Modeling\n",
    "* Board games inherently have states (board configurations) and transitions (moves). A graph is a natural representation where each node represents a board configuration (state), and each edge represents a move (transition) leading to another board configuration.\n",
    "* The Q-value associated with an edge (transition) provides information about the expected future reward of making a particular move from one state to another.\n",
    "\n",
    "State Exploration and Storage:\n",
    "* As the game is played and different states are encountered, the graph grows. Only the states that have been visited are stored, which can be memory-efficient when compared to storing a value for every possible state in a table or array, especially when many states are never visited.\n",
    "* If a state has already been visited, it can be found in the graph, and its associated value and policy can be retrieved. If it hasn't been visited, a new node can be added.\n",
    "\n",
    "Q-value Update\n",
    "* $\\hat{Q}_\\pi(s, a)=$ average of $u_t$ where $s_{t-1}=s, a_t=a$\n",
    "* Algebraic Trick\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{On each $(s, a, u)$ :} \\\\\n",
    "& \\eta=\\frac{1}{1+(\\# \\text { updates to }(s, a))} \\\\\n",
    "& \\hat{Q}_\\pi(s, a) \\leftarrow(1-\\eta) \\hat{Q}_\\pi(s, a)+\\eta u\n",
    "\\end{aligned}\n",
    "$$\n",
    "* SARSA\n",
    "  \n",
    "  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{On each $\\left(s, a, r, s^{\\prime}, a^{\\prime}\\right)$ :} \\\\\n",
    "& \\hat{Q}_\\pi(s, a) \\leftarrow(1-\\eta) \\hat{Q}_\\pi(s, a)+\\eta[\\underbrace{r}_{\\text {data }}+\\gamma \\underbrace{\\hat{Q}_\\pi\\left(s^{\\prime}, a^{\\prime}\\right)}_{\\text {estimate }}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2: Test the working of your **Q-Learning algorithm** on Tic-tac-toe on board with size $5 \\times 5$ or 7x7. \n",
    "\n",
    "+ To make your solution efficient, you are expected to code Monte-Carlo Tree Search algorithm and integrate it with Monte Carlo (or Q-Learning) algorithm that you coded in Task 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3: You are expected to record a short video ideally 5-8 minutes explaining your code along with salient features of the algorithm that you have implemented.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
